# openLLM

## LLM
| Model | Institution | Paper | Github | Demo | 
| --- | --- | --- | --- |  --- |
| chatGPT | OpenAI |  |  |  |
| ChatGLM | 清华&智谱AI |  | https://github.com/THUDM/ChatGLM-6B | https://chatglm.cn/ |
| LLaMA | Meta | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1) | https://github.com/facebookresearch/llama |  |
| PaLM | Google | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) | https://github.com/lucidrains/PaLM-pytorch |  |  
| Claude | Anthropic |  |  | https://www.anthropic.com/index/introducing-claude |
| 星火认知大模型 | 科大讯飞 |  |  | https://xinghuo.xfyun.cn/ |
| BELLE-13B | 链家 |  | https://github.com/LianjiaTech/BELLE |  |
| MOSS-16B | 复旦 |  | https://github.com/OpenLMLab/MOSS |  |
| Vicuna-13B |  |  | https://github.com/lm-sys/FastChat | https://chat.lmsys.org/ |
| Alpaca-13b |  |  | https://github.com/tatsu-lab/stanford_alpaca | https://alpaca-ai.ngrok.io/ |


## Multimodal LLM
| Model | Institution | Paper | Github | Demo | 
| --- | --- | --- | --- |  --- |
| GPT4 | OpenAI |  |  |  |
| LLaVa | University of Wisconsin–Madison | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | https://github.com/haotian-liu/LLaVA | https://llava.hliu.cc/ |
| MiniGPT-4 |  | [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) | https://github.com/Vision-CAIR/MiniGPT-4 | https://huggingface.co/spaces/Vision-CAIR/minigpt4 |
| KOSMOS-1 | Microsoft | [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045) | https://github.com/microsoft/unilm |  |
| MM-REACT | Microsoft | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381) | https://github.com/microsoft/MM-REACT | https://huggingface.co/spaces/microsoft-cognitive-service/mm-react |
| Visual ChatGPT | Microsoft | [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671) | https://github.com/microsoft/visual-chatgpt |  |
| VisionLLM | OpenGVLab | [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175) | https://github.com/OpenGVLab/VisionLLM |  |
| InternGPT  | OpenGVLab | [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/abs/2305.05662) | https://github.com/OpenGVLab/InternGPT | https://igpt.opengvlab.com/ |
| VisualGLM-6B | 清华&智谱AI |  | https://github.com/THUDM/VisualGLM-6B | https://huggingface.co/spaces/lykeven/visualglm-6b |
| BLIP-2 | Salesforce | [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | https://github.com/salesforce/LAVIS/tree/main/projects/blip2 |  |
| Flamingo | DeepMind | [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) | https://github.com/lucidrains/flamingo-pytorch |  |

## Benchmarking LLMs
[中文通用大模型综合性基准SuperCLUE](https://github.com/CLUEbenchmark/SuperCLUE) \
[Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings](https://lmsys.org/blog/2023-05-03-arena/)

## Paper
[Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) \
[Palm: Scaling language modeling with pathways](https://arxiv.org/abs/2204.02311) \
[Pali: A jointly-scaled multilingual language-image model](https://arxiv.org/pdf/2209.06794.pdf)
[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) \
[Multimodal chain-of-thought reasoning in language models](https://arxiv.org/pdf/2302.00923.pdf) \
[Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971v1) \
[Language is not all you need: Aligning perception with language models](https://arxiv.org/abs/2302.14045) \
[Visual chatgpt: Talking, drawing and editing with visual foundation models](https://arxiv.org/abs/2303.04671) \
[Mm-react: Prompting chatgpt for multimodal reasoning and action](https://arxiv.org/abs/2303.11381) \
[Visual instruction tuning](https://arxiv.org/abs/2304.08485) \
[Minigpt-4: Enhancing vision-language understanding with advanced large language models](https://arxiv.org/abs/2304.10592) \
[InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/abs/2305.05662) \
[VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)


